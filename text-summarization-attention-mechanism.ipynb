{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/aaroha33/text-summarization-attention-mechanism?scriptVersionId=84250271\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"<font size=\"+5\" color=Green > <b> <center><u>\n    <br>Text Summarization \n    <br>Sequenece to Sequence Modelling\n    <br>Attention Mechanism </u> </font>","metadata":{}},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#import all the required libraries\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom statistics import mode\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.stem import LancasterStemmer\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import models\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer \nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,Attention\nfrom sklearn.model_selection import train_test_split\nfrom bs4 import BeautifulSoup\n\nimport warnings\npd.set_option(\"display.max_colwidth\", 200)\nwarnings.filterwarnings(\"ignore\")\n\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:45:47.176883Z","iopub.execute_input":"2022-01-02T15:45:47.177318Z","iopub.status.idle":"2022-01-02T15:45:47.188194Z","shell.execute_reply.started":"2022-01-02T15:45:47.177272Z","shell.execute_reply":"2022-01-02T15:45:47.187378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Parse the Data","metadata":{}},{"cell_type":"markdown","source":"We’ll take a sample of 100,000 reviews to reduce the training time of our model.","metadata":{}},{"cell_type":"code","source":"#read the dataset file for text Summarizer\ndf=pd.read_csv(\"../input/amazon-fine-food-reviews/Reviews.csv\",nrows=10000)\n# df = pd.read_csv(\"../input/amazon-fine-food-reviews/Reviews.csv\")\n#drop the duplicate and na values from the records\ndf.drop_duplicates(subset=['Text'],inplace=True)\ndf.dropna(axis=0,inplace=True) #dropping na\ninput_data = df.loc[:,'Text']\ntarget_data = df.loc[:,'Summary']\ntarget_data.replace('', np.nan, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:45:47.190049Z","iopub.execute_input":"2022-01-02T15:45:47.19056Z","iopub.status.idle":"2022-01-02T15:45:47.287415Z","shell.execute_reply.started":"2022-01-02T15:45:47.190416Z","shell.execute_reply":"2022-01-02T15:45:47.286616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:45:47.288823Z","iopub.execute_input":"2022-01-02T15:45:47.289275Z","iopub.status.idle":"2022-01-02T15:45:47.30923Z","shell.execute_reply.started":"2022-01-02T15:45:47.289241Z","shell.execute_reply":"2022-01-02T15:45:47.308416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Summary'][:10]","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:45:47.311086Z","iopub.execute_input":"2022-01-02T15:45:47.311659Z","iopub.status.idle":"2022-01-02T15:45:47.318632Z","shell.execute_reply.started":"2022-01-02T15:45:47.311617Z","shell.execute_reply":"2022-01-02T15:45:47.317768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Text'][:10]","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:45:47.319977Z","iopub.execute_input":"2022-01-02T15:45:47.320689Z","iopub.status.idle":"2022-01-02T15:45:47.332525Z","shell.execute_reply.started":"2022-01-02T15:45:47.320648Z","shell.execute_reply":"2022-01-02T15:45:47.331499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Performing basic preprocessing steps is very important before we get to the model building part. Using messy and uncleaned text data is a potentially disastrous move. So in this step, we will drop all the unwanted symbols, characters, etc. from the text that do not affect the objective of our problem.\n\nHere is the dictionary that we will use for expanding the contractions:","metadata":{}},{"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n                           \"you're\": \"you are\", \"you've\": \"you have\"}","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:45:47.334129Z","iopub.execute_input":"2022-01-02T15:45:47.334932Z","iopub.status.idle":"2022-01-02T15:45:47.350016Z","shell.execute_reply.started":"2022-01-02T15:45:47.334895Z","shell.execute_reply":"2022-01-02T15:45:47.349291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use the contraction using two method, one we can use the above dictionary or we can keep the contraction file as a data set and import it. ","metadata":{}},{"cell_type":"code","source":"input_texts=[]  # Text column\ntarget_texts=[] # summary column\ninput_words=[]\ntarget_words=[]\n# contractions=pickle.load(open(\"../input/contraction/contractions.pkl\",\"rb\"))['contractions']\ncontractions = contraction_mapping\n\n#initialize stop words and LancasterStemmer\nstop_words=set(stopwords.words('english'))\nstemm=LancasterStemmer()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:45:47.430692Z","iopub.execute_input":"2022-01-02T15:45:47.430934Z","iopub.status.idle":"2022-01-02T15:45:47.441811Z","shell.execute_reply.started":"2022-01-02T15:45:47.430908Z","shell.execute_reply":"2022-01-02T15:45:47.441068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"def clean(texts,src):\n  texts = BeautifulSoup(texts, \"lxml\").text   #remove the html tags\n  words=word_tokenize(texts.lower())  #tokenize the text into words \n  #filter words which contains \\ \n  #integers or their length is less than or equal to 3\n  words= list(filter(lambda w:(w.isalpha() and len(w)>=3),words))\n  #contraction file to expand shortened words\n  words= [contractions[w] if w in contractions else w for w in words ]\n\n  #stem the words to their root word and filter stop words\n  if src==\"inputs\":\n    words= [stemm.stem(w) for w in words if w not in stop_words]\n  else:\n    words= [w for w in words if w not in stop_words]\n  return words","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:45:47.444478Z","iopub.execute_input":"2022-01-02T15:45:47.444838Z","iopub.status.idle":"2022-01-02T15:45:47.452654Z","shell.execute_reply.started":"2022-01-02T15:45:47.444797Z","shell.execute_reply":"2022-01-02T15:45:47.451938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pass the input records and target records\nfor in_txt,tr_txt in zip(input_data,target_data):\n  in_words= clean(in_txt,\"inputs\")\n  input_texts+= [' '.join(in_words)]\n  input_words+= in_words\n  #add 'sos' at start and 'eos' at end of text\n  tr_words= clean(\"sos \"+tr_txt+\" eos\",\"target\")\n  target_texts+= [' '.join(tr_words)]\n  target_words+= tr_words","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:45:47.454065Z","iopub.execute_input":"2022-01-02T15:45:47.455031Z","iopub.status.idle":"2022-01-02T15:46:11.956491Z","shell.execute_reply.started":"2022-01-02T15:45:47.454958Z","shell.execute_reply":"2022-01-02T15:46:11.955744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#store only unique words from input and target list of words\ninput_words = sorted(list(set(input_words)))\ntarget_words = sorted(list(set(target_words)))\nnum_in_words = len(input_words) #total number of input words\nnum_tr_words = len(target_words) #total number of target words\n \n#get the length of the input and target texts which appears most often  \nmax_in_len = mode([len(i) for i in input_texts])\nmax_tr_len = mode([len(i) for i in target_texts])\n \nprint(\"number of input words : \",num_in_words)\nprint(\"number of target words : \",num_tr_words)\nprint(\"maximum input length : \",max_in_len)\nprint(\"maximum target length : \",max_tr_len)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:46:11.957777Z","iopub.execute_input":"2022-01-02T15:46:11.958056Z","iopub.status.idle":"2022-01-02T15:46:12.019184Z","shell.execute_reply.started":"2022-01-02T15:46:11.95802Z","shell.execute_reply":"2022-01-02T15:46:12.017686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split it","metadata":{}},{"cell_type":"code","source":"#split the input and target text into 80:20 ratio or testing size of 20%.\nx_train,x_test,y_train,y_test=train_test_split(input_texts,target_texts,test_size=0.2,random_state=0) ","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:47:42.0533Z","iopub.execute_input":"2022-01-02T15:47:42.053815Z","iopub.status.idle":"2022-01-02T15:47:42.068056Z","shell.execute_reply.started":"2022-01-02T15:47:42.05378Z","shell.execute_reply":"2022-01-02T15:47:42.067364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train the tokenizer with all the words\nin_tokenizer = Tokenizer()\nin_tokenizer.fit_on_texts(x_train)\ntr_tokenizer = Tokenizer()\ntr_tokenizer.fit_on_texts(y_train)\n \n#convert text into sequence of integers\n#where the integer will be the index of that word\nx_train= in_tokenizer.texts_to_sequences(x_train) \ny_train= tr_tokenizer.texts_to_sequences(y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:47:47.756705Z","iopub.execute_input":"2022-01-02T15:47:47.756984Z","iopub.status.idle":"2022-01-02T15:47:48.491624Z","shell.execute_reply.started":"2022-01-02T15:47:47.756956Z","shell.execute_reply":"2022-01-02T15:47:48.49092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pad array of 0's if the length is less than the maximum length \nen_in_data= pad_sequences(x_train,  maxlen=max_in_len, padding='post') \ndec_data= pad_sequences(y_train,  maxlen=max_tr_len, padding='post')\n \n#decoder input data will not include the last word \n#i.e. 'eos' in decoder input data\ndec_in_data = dec_data[:,:-1]\n#decoder target data will be one time step ahead as it will not include\n# the first word i.e 'sos'\ndec_tr_data = dec_data.reshape(len(dec_data),max_tr_len,1)[:,1:]","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:47:53.630216Z","iopub.execute_input":"2022-01-02T15:47:53.63085Z","iopub.status.idle":"2022-01-02T15:47:53.790133Z","shell.execute_reply.started":"2022-01-02T15:47:53.630813Z","shell.execute_reply":"2022-01-02T15:47:53.78938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"K.clear_session() \nlatent_dim = 500\n \n#create input object of total number of encoder words\nen_inputs = Input(shape=(max_in_len,)) \nen_embedding = Embedding(num_in_words+1, latent_dim)(en_inputs) ","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:47:58.343674Z","iopub.execute_input":"2022-01-02T15:47:58.344392Z","iopub.status.idle":"2022-01-02T15:48:00.647634Z","shell.execute_reply.started":"2022-01-02T15:47:58.344346Z","shell.execute_reply":"2022-01-02T15:48:00.646917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create 3 stacked LSTM layer with the shape of hidden dimension for text summarizer using deep learning\n#LSTM 1\nen_lstm1= LSTM(latent_dim, return_state=True, return_sequences=True) \nen_outputs1, state_h1, state_c1= en_lstm1(en_embedding) \n \n#LSTM2\nen_lstm2= LSTM(latent_dim, return_state=True, return_sequences=True) \nen_outputs2, state_h2, state_c2= en_lstm2(en_outputs1) \n \n#LSTM3\nen_lstm3= LSTM(latent_dim,return_sequences=True,return_state=True)\nen_outputs3 , state_h3 , state_c3= en_lstm3(en_outputs2)\n \n#encoder states\nen_states= [state_h3, state_c3]","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:48:07.011815Z","iopub.execute_input":"2022-01-02T15:48:07.012596Z","iopub.status.idle":"2022-01-02T15:48:08.200953Z","shell.execute_reply.started":"2022-01-02T15:48:07.012547Z","shell.execute_reply":"2022-01-02T15:48:08.200228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create 3 stacked LSTM layer with the shape of hidden dimension for text summarizer using deep learning\n#LSTM 1\nen_lstm1= LSTM(latent_dim, return_state=True, return_sequences=True) \nen_outputs1, state_h1, state_c1= en_lstm1(en_embedding) \n \n#LSTM2\nen_lstm2= LSTM(latent_dim, return_state=True, return_sequences=True) \nen_outputs2, state_h2, state_c2= en_lstm2(en_outputs1) \n \n#LSTM3\nen_lstm3= LSTM(latent_dim,return_sequences=True,return_state=True)\nen_outputs3 , state_h3 , state_c3= en_lstm3(en_outputs2)\n \n#encoder states\nen_states= [state_h3, state_c3]","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:48:23.856339Z","iopub.execute_input":"2022-01-02T15:48:23.856596Z","iopub.status.idle":"2022-01-02T15:48:24.749662Z","shell.execute_reply.started":"2022-01-02T15:48:23.856568Z","shell.execute_reply":"2022-01-02T15:48:24.74893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decoder","metadata":{}},{"cell_type":"code","source":"# Decoder. \ndec_inputs = Input(shape=(None,)) \ndec_emb_layer = Embedding(num_tr_words+1, latent_dim) \ndec_embedding = dec_emb_layer(dec_inputs) \n \n#initialize decoder's LSTM layer with the output states of encoder\ndec_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndec_outputs, *_ = dec_lstm(dec_embedding,initial_state=en_states) ","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:48:30.357809Z","iopub.execute_input":"2022-01-02T15:48:30.358075Z","iopub.status.idle":"2022-01-02T15:48:30.666027Z","shell.execute_reply.started":"2022-01-02T15:48:30.358046Z","shell.execute_reply":"2022-01-02T15:48:30.66529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention Layer","metadata":{}},{"cell_type":"code","source":"#Attention layer\nattention =Attention()\nattn_out = attention([dec_outputs,en_outputs3])\n \n#Concatenate the attention output with the decoder outputs\nmerge=Concatenate(axis=-1, name='concat_layer1')([dec_outputs,attn_out])","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:48:35.181956Z","iopub.execute_input":"2022-01-02T15:48:35.182509Z","iopub.status.idle":"2022-01-02T15:48:35.197134Z","shell.execute_reply.started":"2022-01-02T15:48:35.182472Z","shell.execute_reply":"2022-01-02T15:48:35.196331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dense layer (output layer)\ndec_dense = Dense(num_tr_words+1, activation='softmax') \ndec_outputs = dec_dense(merge)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:48:41.140348Z","iopub.execute_input":"2022-01-02T15:48:41.140811Z","iopub.status.idle":"2022-01-02T15:48:41.174469Z","shell.execute_reply.started":"2022-01-02T15:48:41.140774Z","shell.execute_reply":"2022-01-02T15:48:41.173798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the Model","metadata":{}},{"cell_type":"code","source":"#Model class and model summary for text Summarizer\nmodel = Model([en_inputs, dec_inputs], dec_outputs) \nmodel.summary()\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:48:45.682569Z","iopub.execute_input":"2022-01-02T15:48:45.682857Z","iopub.status.idle":"2022-01-02T15:48:46.482939Z","shell.execute_reply.started":"2022-01-02T15:48:45.682828Z","shell.execute_reply":"2022-01-02T15:48:46.482174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"] ) \nhistory = model.fit( \n                    [en_in_data, dec_in_data],\n                    dec_tr_data, \n                    batch_size=512, \n                    epochs=10, \n                    validation_split=0.1,)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:55:49.300119Z","iopub.execute_input":"2022-01-02T15:55:49.300448Z","iopub.status.idle":"2022-01-02T15:56:35.905683Z","shell.execute_reply.started":"2022-01-02T15:55:49.300411Z","shell.execute_reply":"2022-01-02T15:56:35.90498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save model\nmodel.save('Text_Summarizer.h5')\nprint('Model Saved!')","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:50:29.582227Z","iopub.execute_input":"2022-01-02T15:50:29.582483Z","iopub.status.idle":"2022-01-02T15:50:29.873958Z","shell.execute_reply.started":"2022-01-02T15:50:29.582448Z","shell.execute_reply":"2022-01-02T15:50:29.873023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-02T15:56:39.971185Z","iopub.execute_input":"2022-01-02T15:56:39.971709Z","iopub.status.idle":"2022-01-02T15:56:40.154488Z","shell.execute_reply.started":"2022-01-02T15:56:39.971672Z","shell.execute_reply":"2022-01-02T15:56:40.153753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Next, let’s build the dictionary to convert the index to word for target and source vocabulary:","metadata":{}},{"cell_type":"markdown","source":"# Inference Model","metadata":{}},{"cell_type":"markdown","source":"### Encoder Inference:","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# encoder inference\nlatent_dim=500\n#/content/gdrive/MyDrive/Text Summarizer/\n#load the model\nmodel = models.load_model(\"Text_Summarizer.h5\")\n \n#construct encoder model from the output of 6 layer i.e.last LSTM layer\nen_outputs,state_h_enc,state_c_enc = model.layers[6].output\nen_states=[state_h_enc,state_c_enc]\n#add input and state from the layer.\nen_model = Model(model.input[0],[en_outputs]+en_states)","metadata":{"execution":{"iopub.status.busy":"2022-01-02T16:09:31.097864Z","iopub.execute_input":"2022-01-02T16:09:31.098537Z","iopub.status.idle":"2022-01-02T16:09:32.541449Z","shell.execute_reply.started":"2022-01-02T16:09:31.098502Z","shell.execute_reply":"2022-01-02T16:09:32.54075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Decoder Inference:","metadata":{}},{"cell_type":"code","source":"# decoder inference\n#create Input object for hidden and cell state for decoder\n#shape of layer with hidden or latent dimension\ndec_state_input_h = Input(shape=(latent_dim,))\ndec_state_input_c = Input(shape=(latent_dim,))\ndec_hidden_state_input = Input(shape=(max_in_len,latent_dim))\n \n# Get the embeddings and input layer from the model\ndec_inputs = model.input[1]\ndec_emb_layer = model.layers[5]\ndec_lstm = model.layers[7]\ndec_embedding= dec_emb_layer(dec_inputs)\n \n#add input and initialize LSTM layer with encoder LSTM states.\ndec_outputs2, state_h2, state_c2 = dec_lstm(dec_embedding, initial_state=[dec_state_input_h,dec_state_input_c])","metadata":{"execution":{"iopub.status.busy":"2022-01-02T16:13:29.989877Z","iopub.execute_input":"2022-01-02T16:13:29.990689Z","iopub.status.idle":"2022-01-02T16:13:30.178445Z","shell.execute_reply.started":"2022-01-02T16:13:29.990651Z","shell.execute_reply":"2022-01-02T16:13:30.177763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Attention Inference:","metadata":{}},{"cell_type":"code","source":"#Attention layer\nattention = model.layers[8]\nattn_out2 = attention([dec_outputs2,dec_hidden_state_input])\n \nmerge2 = Concatenate(axis=-1)([dec_outputs2, attn_out2])","metadata":{"execution":{"iopub.status.busy":"2022-01-02T16:14:03.620795Z","iopub.execute_input":"2022-01-02T16:14:03.621064Z","iopub.status.idle":"2022-01-02T16:14:03.635149Z","shell.execute_reply.started":"2022-01-02T16:14:03.621034Z","shell.execute_reply":"2022-01-02T16:14:03.634408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dense layer","metadata":{}},{"cell_type":"code","source":"#Dense layer\ndec_dense = model.layers[10]\ndec_outputs2 = dec_dense(merge2)\n \n# Finally define the Model Class\ndec_model = Model(\n[dec_inputs] + [dec_hidden_state_input,dec_state_input_h,dec_state_input_c],\n[dec_outputs2] + [state_h2, state_c2])","metadata":{"execution":{"iopub.status.busy":"2022-01-02T16:37:54.510492Z","iopub.execute_input":"2022-01-02T16:37:54.511232Z","iopub.status.idle":"2022-01-02T16:37:54.535939Z","shell.execute_reply.started":"2022-01-02T16:37:54.511196Z","shell.execute_reply":"2022-01-02T16:37:54.535287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create a dictionary with a key as index and value as words.\nreverse_target_word_index = tr_tokenizer.index_word\nreverse_source_word_index = in_tokenizer.index_word\ntarget_word_index = tr_tokenizer.word_index","metadata":{"execution":{"iopub.status.busy":"2022-01-02T16:40:14.157865Z","iopub.execute_input":"2022-01-02T16:40:14.158454Z","iopub.status.idle":"2022-01-02T16:40:14.16207Z","shell.execute_reply.started":"2022-01-02T16:40:14.158414Z","shell.execute_reply":"2022-01-02T16:40:14.161334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_sequence(input_seq):\n    # get the encoder output and states by passing the input sequence\n    en_out, en_h, en_c = en_model.predict(input_seq)\n\n    # target sequence with inital word as 'sos'\n    target_seq = np.zeros((1, 1))\n    target_seq[0, 0] = target_word_index['sos']\n\n    # if the iteration reaches the end of text than it will be stop the iteration\n    stop_condition = False\n    # append every predicted word in decoded sentence\n    decoded_sentence = \"\"\n    while not stop_condition:\n        # get predicted output, hidden and cell state.\n        output_words, dec_h, dec_c = dec_model.predict([target_seq] + [en_out, en_h, en_c])\n\n        # get the index and from the dictionary get the word for that index.\n        word_index = np.argmax(output_words[0, -1, :])\n        text_word = reverse_target_word_index[word_index]\n        decoded_sentence += text_word + \" \"\n\n        # Exit condition: either hit max length\n        # or find a stop word or last word.\n        if text_word == \"eos\" or len(decoded_sentence) > max_tr_len:\n            stop_condition = True\n\n        # update target sequence to the current word index.\n        target_seq = np.zeros((1, 1))\n        target_seq[0, 0] = word_index\n        en_h, en_c = dec_h, dec_c\n\n    # return the deocded sentence\n    return decoded_sentence","metadata":{"execution":{"iopub.status.busy":"2022-01-02T16:50:52.513535Z","iopub.execute_input":"2022-01-02T16:50:52.513803Z","iopub.status.idle":"2022-01-02T16:50:52.521653Z","shell.execute_reply.started":"2022-01-02T16:50:52.513773Z","shell.execute_reply":"2022-01-02T16:50:52.520822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp_review = input(\"Enter : \")\nprint(\"Review :\", inp_review)\n\ninp_review = clean(inp_review, \"inputs\")\ninp_review = ' '.join(inp_review)\ninp_x = in_tokenizer.texts_to_sequences([inp_review])\ninp_x = pad_sequences(inp_x, maxlen=max_in_len, padding='post')\n\nsummary = decode_sequence(inp_x.reshape(1, max_in_len))\nif 'eos' in summary:\n    summary = summary.replace('eos', '')\nprint(\"\\nPredicted summary:\", summary);\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-01-02T16:53:01.73694Z","iopub.execute_input":"2022-01-02T16:53:01.737284Z","iopub.status.idle":"2022-01-02T16:53:06.588177Z","shell.execute_reply.started":"2022-01-02T16:53:01.737255Z","shell.execute_reply":"2022-01-02T16:53:06.587357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}